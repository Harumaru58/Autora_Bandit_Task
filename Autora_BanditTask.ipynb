{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# preliminaries\n",
        "\n",
        "\n",
        "*   Installations\n",
        "*   Imports\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gRC-u0m5veye"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r /content/drive/MyDrive/IKW/Autora_Bandit/req.txt"
      ],
      "metadata": {
        "id": "CmIrBw8yj-ST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TO do list:\n",
        "\n",
        "\n",
        "*   run-1 : when γ=0\n",
        "*   run-2 : when γ!=0 but is very small\n",
        "*   run-3 : when γ is not small\n",
        "*   run-4 : Define another theorist instead of SINDy part\n",
        "*   Reform Experimentalist\n",
        "\n"
      ],
      "metadata": {
        "id": "RoQqCupkYy3N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# *** IMPORTS *** #\n",
        "\n",
        "# Python Core\n",
        "from dataclasses import dataclass, field\n",
        "from typing import List\n",
        "import random, json\n",
        "\n",
        "# External Vendors\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.base import BaseEstimator\n",
        "import torch\n",
        "\n",
        "# General Aut oRA\n",
        "from autora.variable import VariableCollection, Variable\n",
        "from autora.state import StandardState, on_state, Delta\n",
        "\"\"\"\n",
        "Bandit Workflow\n",
        "    Reward Trajectory as Conditions\n",
        "    Theorist: Rnn Sindy Theorist\n",
        "    Experimentalist: Random Sampling + Model Disagreement\n",
        "    Runner: Synthetic + Firebase Runner + Prolific recruitment)\n",
        "\"\"\"\n",
        "\n",
        "# *** IMPORTS *** #\n",
        "\n",
        "# Python Core\n",
        "from dataclasses import dataclass, field\n",
        "from typing import List\n",
        "import random, json\n",
        "\n",
        "# External Vendors\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.base import BaseEstimator\n",
        "import torch\n",
        "\n",
        "# General AutoRA\n",
        "from autora.variable import VariableCollection, Variable\n",
        "from autora.state import StandardState, on_state, Delta\n",
        "\n",
        "# Experimentalists\n",
        "from autora.experimentalist.bandit_random import bandit_random_pool\n",
        "from autora.experimentalist.model_disagreement import model_disagreement_sampler_custom_distance\n",
        "\n",
        "# Experiment Runner\n",
        "from autora.experiment_runner.synthetic.psychology.q_learning import q_learning\n",
        "from autora.experiment_runner.firebase_prolific import firebase_runner, firebase_prolific_runner\n",
        "\n",
        "# Theorist\n",
        "from autora.theorist.rnn_sindy_rl import RNNSindy\n",
        "from autora.theorist.rnn_sindy_rl.utils.parse import parse as parse_equation\n",
        "RUNNER_TYPE = 'synthetic'\n",
        "\n",
        "TRIALS_PER_PARTICIPANTS = 100\n",
        "SAMPLES_PER_CYCLE = 1\n",
        "PARTICIPANTS_PER_CYCLE = 40\n",
        "CYCLES = 4\n",
        "INITIAL_REWARD_PROBABILITY_RANGE = [.2, .8]\n",
        "SIGMA_RANGE = [.2, .2]\n",
        "\n",
        "EPOCHS = 10 # 100\n",
        "\n",
        "\n",
        "\n",
        "seed = 11\n",
        "\n",
        "# for reproducible results:\n",
        "if seed is not None:\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "# *** AUTORA SETUP *** #\n",
        "\n",
        "# ** Set up variables ** #\n",
        "# independent variable is \"reward-trajectory\": A 2 x n_trials Vector with entries between 0 and 1\n",
        "# dependent variable is \"choice-trajectory\": A 2 x n_trials Vector with boolean entries (one hot encoded)\n",
        "\n",
        "variables = VariableCollection(\n",
        "    independent_variables=[Variable(name=\"reward-trajectory\")\n",
        "    ,Variable(name='previous-choice-trajectory')\n",
        "    ],\n",
        "    dependent_variables=[Variable(name=\"choice-trajectory\")]\n",
        ")\n",
        "\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class RnnState(StandardState):\n",
        "    models_additional:  List[BaseEstimator] = field(\n",
        "        default_factory=list,\n",
        "        metadata={\"delta\": \"extend\"},\n",
        "    )\n",
        "\n",
        "# initialize the state:\n",
        "state = RnnState(variables=variables)\n",
        "\n",
        "@on_state()\n",
        "def pool_on_state(num_samples, n_trials=TRIALS_PER_PARTICIPANTS): ##This only defines the rewards, and not the choices##\n",
        "    \"\"\"\n",
        "    This is creates `num_samples` randomized reward-trajectories of length `n_trials`\n",
        "    \"\"\"\n",
        "    sigma = np.random.uniform(SIGMA_RANGE[0], SIGMA_RANGE[1])\n",
        "    trajectory_array = bandit_random_pool(\n",
        "        num_rewards=2,\n",
        "        sequence_length=n_trials,\n",
        "        initial_probabilities=[INITIAL_REWARD_PROBABILITY_RANGE, INITIAL_REWARD_PROBABILITY_RANGE],\n",
        "        sigmas=[sigma, sigma],\n",
        "        num_samples=num_samples\n",
        "    )\n",
        "    trajectory_df = pd.DataFrame({'reward-trajectory': trajectory_array})\n",
        "    return Delta(conditions=trajectory_df)\n",
        "\n",
        "\n",
        "def custom_distance(prob_array_a, prob_array_b):\n",
        "    return np.mean([(prob_array_a[0] - prob_array_b[0])**2 + (prob_array_a[1] - prob_array_b[1])**2])\n",
        "\n",
        "###############  Experimentalist ##########################\n",
        "\n",
        "@on_state()\n",
        "def model_disagreement_on_state(\n",
        "        conditions, models, models_additional, num_samples):\n",
        "    conditions = model_disagreement_sampler_custom_distance(\n",
        "        conditions=conditions['reward-trajectory'],\n",
        "        models=[models[-1], models_additional[-1]],\n",
        "        distance_fct=custom_distance,\n",
        "        num_samples=num_samples,\n",
        "    )\n",
        "    return Delta(conditions=conditions)\n",
        "################# DATA ##########################\n",
        "runner = q_learning()\n",
        "\n",
        "@on_state()\n",
        "def runner_on_state_synthetic(conditions):\n",
        "    choices, choice_probabilities = runner.run(conditions, return_choice_probabilities=True)\n",
        "    # print(choices)\n",
        "    experiment_data = pd.DataFrame({\n",
        "        'reward-trajectory': conditions['reward-trajectory'].tolist(),\n",
        "        'choice-trajectory': choices,\n",
        "        'previous-choice-trajectory': [np.concatenate((np.array([[0,0]]),choices[0][:-1]))],\n",
        "        'choice-probability-trajectory': choice_probabilities\n",
        "    })\n",
        "    return Delta(experiment_data=experiment_data)\n",
        "\n",
        "####################   Theorist   ####################\n",
        "\n",
        "theorist = RNNSindy(2, epochs=EPOCHS, polynomial_degree=2)\n",
        "theorist_additional = RNNSindy(2, epochs=EPOCHS, polynomial_degree=1)\n",
        "\n",
        "@on_state()\n",
        "def theorist_on_state(experiment_data):\n",
        "    # x = [np.array(experiment_data[['reward-trajectory']]),np.array(experiment_data[['previous-choice-trajectory']])]#\n",
        "    x = experiment_data['reward-trajectory']\n",
        "    y = experiment_data['choice-trajectory']\n",
        "    return Delta(models=[theorist.fit(x, y)])\n",
        "\n",
        "\n",
        "@on_state()\n",
        "def theorist_additional_on_state(experiment_data):\n",
        "    x = experiment_data['reward-trajectory']\n",
        "    y = experiment_data['choice-trajectory']\n",
        "    return Delta(models_additional=[theorist_additional.fit(x, y)])\n",
        "\n",
        "\n",
        "################ LOOP #########################\n",
        "\n",
        "\n",
        "for c in range(1, CYCLES + 1):\n",
        "\n",
        "    if len(state.models) > 0:\n",
        "        state = pool_on_state(state, num_samples=20)\n",
        "        state = model_disagreement_on_state(state, num_samples=SAMPLES_PER_CYCLE)\n",
        "    else:\n",
        "        state = pool_on_state(state, num_samples=SAMPLES_PER_CYCLE)\n",
        "\n",
        "    if RUNNER_TYPE == 'synthetic':\n",
        "        state = runner_on_state_synthetic(state)\n",
        "\n",
        "    state = theorist_on_state(state)\n",
        "    state = theorist_additional_on_state(state)\n",
        "\n",
        "    model = state.models[-1]\n",
        "    model_additional = state.models_additional[-1]\n",
        "\n",
        "\n",
        "    equations_model = parse_equation(model)\n",
        "    equation_model_additional = parse_equation(model_additional)\n",
        "\n",
        "    print('# MODEL DEGREE = 2#')\n",
        "    print(f'chosen: {equations_model[0]}')\n",
        "    print(f'non chosen: {equations_model[1]}')\n",
        "\n",
        "    print('# MODEL DEGREE = 1#')\n",
        "    print(f'chosen: {equation_model_additional[0]}')\n",
        "    print(f'non chosen: {equation_model_additional[1]}')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x9EJL3IMc6Oc",
        "outputId": "66c494fc-4e7c-49a8-ef54-b020994d0e61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training the RNN...\n",
            "Epoch 1/10 --- Loss: 2.6254387; Time: 1.5581s; Convergence value: 1.63e+00\n",
            "Epoch 2/10 --- Loss: 1.5450935; Time: 1.5929s; Convergence value: 1.34e+00\n",
            "Epoch 3/10 --- Loss: 1.1207993; Time: 2.1299s; Convergence value: 1.02e+00\n",
            "Epoch 4/10 --- Loss: 0.5710946; Time: 2.3428s; Convergence value: 8.86e-01\n",
            "Epoch 5/10 --- Loss: 0.2770328; Time: 1.7280s; Convergence value: 7.49e-01\n",
            "Epoch 6/10 --- Loss: 0.2338850; Time: 1.5957s; Convergence value: 6.07e-01\n",
            "Epoch 7/10 --- Loss: 0.1286141; Time: 1.5927s; Convergence value: 5.15e-01\n",
            "Epoch 8/10 --- Loss: 0.1880593; Time: 1.5883s; Convergence value: 4.37e-01\n",
            "Epoch 9/10 --- Loss: 0.1537335; Time: 1.5860s; Convergence value: 3.71e-01\n",
            "Epoch 10/10 --- Loss: 0.1805673; Time: 1.5851s; Convergence value: 2.30e-01\n",
            "Maximum number of training epochs reached.\n",
            "Model did not converge yet.\n",
            "Test the trained RNN on a test dataset...\n",
            "Epoch 1/1 --- Loss: 0.1470104; Time: 0.1356s; Convergence value: nan\n",
            "Maximum number of training epochs reached.\n",
            "Model did not converge yet.\n",
            "RNN training took 17.44 seconds.\n",
            "\n",
            "SINDy model for xQf:\n",
            " Iteration ... |y - Xw|^2 ...  a * |w|_2 ...      |w|_0 ... Total error: |y - Xw|^2 + a * |w|_2\n",
            "         0 ... 2.2928e-02 ... 4.6278e-02 ...          2 ... 6.9206e-02\n",
            "         1 ... 1.2248e-02 ... 4.1469e-02 ...          2 ... 5.3717e-02\n",
            "(xQf)[k+1] = 0.586 xQf[k] + 0.279 xQf[k]^2\n",
            "\n",
            "SINDy model for xQr:\n",
            " Iteration ... |y - Xw|^2 ...  a * |w|_2 ...      |w|_0 ... Total error: |y - Xw|^2 + a * |w|_2\n",
            "         0 ... 1.3193e-02 ... 4.7832e-02 ...          4 ... 6.1026e-02\n",
            "         1 ... 8.6857e-03 ... 4.7772e-02 ...          4 ... 5.6458e-02\n",
            "(xQr)[k+1] = 0.152 1 + 0.807 xQr[k] + -0.023 xQr[k]^2 + 0.107 xQr[k] cr[k]\n",
            "\n",
            "SINDy model for xQc:\n",
            " Iteration ... |y - Xw|^2 ...  a * |w|_2 ...      |w|_0 ... Total error: |y - Xw|^2 + a * |w|_2\n",
            "         0 ... 5.4623e-03 ... 5.0751e-02 ...          5 ... 5.6214e-02\n",
            "         1 ... 5.3478e-03 ... 5.0860e-02 ...          5 ... 5.6208e-02\n",
            "(xQc)[k+1] = 0.044 1 + 0.761 xQc[k] + 0.282 cQr[k] + 0.027 xQc[k]^2 + -0.073 xQc[k] cQr[k]\n",
            "Beta for SINDy: 2.3745412826538086\n",
            "Training the RNN...\n",
            "Epoch 1/10 --- Loss: 0.0000000; Time: 2.1911s; Convergence value: 1.00e+00\n",
            "Epoch 2/10 --- Loss: 0.0000046; Time: 2.3625s; Convergence value: 4.84e-01\n",
            "Epoch 3/10 --- Loss: 0.0000697; Time: 1.6662s; Convergence value: 3.11e-01\n",
            "Epoch 4/10 --- Loss: 0.0025720; Time: 1.5709s; Convergence value: 2.25e-01\n",
            "Epoch 5/10 --- Loss: 0.0043026; Time: 1.6123s; Convergence value: 1.72e-01\n",
            "Epoch 6/10 --- Loss: 0.0096722; Time: 1.5656s; Convergence value: 1.38e-01\n",
            "Epoch 7/10 --- Loss: 0.0249784; Time: 1.5982s; Convergence value: 1.14e-01\n",
            "Epoch 8/10 --- Loss: 0.0074848; Time: 1.5469s; Convergence value: 9.64e-02\n",
            "Epoch 9/10 --- Loss: 0.1674338; Time: 2.0050s; Convergence value: 1.03e-01\n",
            "Epoch 10/10 --- Loss: 0.0178877; Time: 2.2883s; Convergence value: 4.95e-02\n",
            "Maximum number of training epochs reached.\n",
            "Model did not converge yet.\n",
            "Test the trained RNN on a test dataset...\n",
            "Epoch 1/1 --- Loss: 0.0453313; Time: 0.2278s; Convergence value: nan\n",
            "Maximum number of training epochs reached.\n",
            "Model did not converge yet.\n",
            "RNN training took 18.64 seconds.\n",
            "\n",
            "SINDy model for xQf:\n",
            " Iteration ... |y - Xw|^2 ...  a * |w|_2 ...      |w|_0 ... Total error: |y - Xw|^2 + a * |w|_2\n",
            "         0 ... 8.7024e-03 ... 3.9010e-02 ...          2 ... 4.7712e-02\n",
            "(xQf)[k+1] = 0.027 1 + 0.738 xQf[k]\n",
            "\n",
            "SINDy model for xQr:\n",
            " Iteration ... |y - Xw|^2 ...  a * |w|_2 ...      |w|_0 ... Total error: |y - Xw|^2 + a * |w|_2\n",
            "         0 ... 2.0142e-03 ... 9.2808e-02 ...          2 ... 9.4822e-02\n",
            "         1 ... 1.9695e-03 ... 9.2738e-02 ...          2 ... 9.4707e-02\n",
            "(xQr)[k+1] = 0.034 1 + 0.974 xQr[k]\n",
            "\n",
            "SINDy model for xQc:\n",
            " Iteration ... |y - Xw|^2 ...  a * |w|_2 ...      |w|_0 ... Total error: |y - Xw|^2 + a * |w|_2\n",
            "         0 ... 1.1851e-02 ... 3.1973e-02 ...          2 ... 4.3824e-02\n",
            "         1 ... 1.3631e-02 ... 2.9820e-02 ...          1 ... 4.3452e-02\n",
            "         2 ... 1.3105e-02 ... 3.0330e-02 ...          1 ... 4.3435e-02\n",
            "(xQc)[k+1] = 0.634 xQc[k]\n",
            "Beta for SINDy: 6.093017101287842\n",
            "sedighe = RNNSindy(epochs=10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Su2qspF_M38v"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}